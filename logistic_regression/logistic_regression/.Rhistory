install.packages(XLConnect)
install.packages("XLConnect")
library(XLConnect)
library("XLConnect")
install.packages("XLConnectJars")
library("XLConnect")
library(XLConnect)
library(XLConnect)
library("XLConnect")
df <- readWorksheetFromFile("refine_original.xlsx",sheet=1)
View(df)
install.packages("readxl")
df
# titanic is avaliable in your workspace
# Check out the structure of titanic
str(titanic)
# Use ggplot() for the first instruction
ggplot(titanic,aes(x = factor(Pclass), fill = factor(Sex))) +
geom_bar(position = "dodge")
# Use ggplot() for the second instruction
ggplot(titanic,aes(x = factor(Pclass), fill = factor(Sex))) +
geom_bar(position = "dodge") +
facet_grid("~Survived")
# Position jitter (use below)
posn.j <- position_jitter(0.5, 0)
# Use ggplot() for the last instruction
ggplot(titanic,aes(x = factor(Pclass), y =Age, col = factor(Sex))) +
geom_jitter(size = 3,  alpha = 0.5, position = posn.j) +
facet_grid("~Survived")
# titanic is avaliable in your workspace
titanic<-read.csv("titanic3_original.csv", na.strings = "")
# Check out the structure of titanic
str(titanic)
# Use ggplot() for the first instruction
install.packages("ggplot2")
ggplot(titanic,aes(x = factor(Pclass), fill = factor(Sex))) +
geom_bar(position = "dodge")
# Use ggplot() for the second instruction
ggplot(titanic,aes(x = factor(Pclass), fill = factor(Sex))) +
geom_bar(position = "dodge") +
facet_grid("~Survived")
# Position jitter (use below)
posn.j <- position_jitter(0.5, 0)
# Use ggplot() for the last instruction
ggplot(titanic,aes(x = factor(Pclass), y =Age, col = factor(Sex))) +
geom_jitter(size = 3,  alpha = 0.5, position = posn.j) +
facet_grid("~Survived")
# titanic is avaliable in your workspace
titanic<-read.csv("titanic3_original.csv", na.strings = "")
# Check out the structure of titanic
str(titanic)
# Use ggplot() for the first instruction
install.packages("ggplot2")
library(ggplot2)
ggplot(titanic,aes(x = factor(Pclass), fill = factor(Sex))) +
geom_bar(position = "dodge")
# Use ggplot() for the second instruction
ggplot(titanic,aes(x = factor(Pclass), fill = factor(Sex))) +
geom_bar(position = "dodge") +
facet_grid("~Survived")
# Position jitter (use below)
posn.j <- position_jitter(0.5, 0)
# Use ggplot() for the last instruction
ggplot(titanic,aes(x = factor(Pclass), y =Age, col = factor(Sex))) +
geom_jitter(size = 3,  alpha = 0.5, position = posn.j) +
facet_grid("~Survived")
#Load data into data frame df and replace blanks with NA
df<-read.csv("titanic3_original.csv", na.strings = "")
#Replace missing data in port of embarkation with 'S'
df$embarked[is.na(df$embarked)]<-"S"
#Replace the empty age cells with mean of age then round the value
df$age[is.na(df$age)]<-mean(df$age,na.rm = TRUE)
df$age <-round(df$age,2)
#Convert boat columns to character and replace NA values with 'None'
df$boat<-as.character(df$boat)
dfdf$boat[is.na(df$boat)]<- "None"
#create a new column 'has_cabin_number with value 1 if there is a cabin number and 0 otherwise
df$has_cabin_number<-ifelse(is.na(df$cabin),0,1)
write.csv(df,"titanic_clean.csv")
#Load data into data frame df and replace blanks with NA
df<-read.csv("titanic3_original.csv", na.strings = "")
#Replace missing data in port of embarkation with 'S'
df$embarked[is.na(df$embarked)]<-"S"
#Replace the empty age cells with mean of age then round the value
df$age[is.na(df$age)]<-mean(df$age,na.rm = TRUE)
df$age <-round(df$age,2)
#Convert boat columns to character and replace NA values with 'None'
df$boat<-as.character(df$boat)
df$boat[is.na(df$boat)]<- "None"
#create a new column 'has_cabin_number with value 1 if there is a cabin number and 0 otherwise
df$has_cabin_number<-ifelse(is.na(df$cabin),0,1)
write.csv(df,"titanic_clean.csv")
#Load data into R
df<-read.csv("refine_original.csv")
#Fix the misspwlling in the company column
df$company<- gsub("\\s","",df$company)
philips <- grep("\\Bips$\\b", df$company, TRUE)
df$company[philips]<- "philips"
akzo <- grep("\\bakz.\\b",df$company,TRUE)
df$company[akzo]<-"akzo"
van<-grep("van\\B", df$company,TRUE)
df$company[van]<-"van houten"
unil <- grep("uni\\B", df$company, TRUE)
df$company[unil]<-"unilever"
library(tidyr)
#Seperate product code and number
df<-separate(df, Product.code...number,c("product_code","product_number"),sep="-")
#Add product category column
df$product_category<-""
#Populate product category
p<-grep("p", df$product_code,TRUE)
df$product_category[p]<-"Smartphone"
v<-grep("v", df$product_code,TRUE)
df$product_category[v]<-"TV"
x<-grep("x", df$product_code,TRUE)
df$product_category[x]<-"Laptop"
q<-grep("q", df$product_code,TRUE)
df$product_category[q]<-"Tablet"
df<-df[c(1,2,8,3,4,5,6,7)]
setwd("C:/Users/Noura/Documents/R/Data Science Foundation/Capstone/trainingSet/trainingSet")
setwd("C:/Users/Noura/Documents/R/Data Science Foundation/Capstone/trainingSet/trainingSet")
setwd("C:/Users/Noura/Documents/R/Data Science Foundation/logistic_regression/logistic_regression")
NH11 <- readRDS("dataSets/NatHealth2011.rds")
labs <- attributes(NH11)$labels
##   [CDC website] http://www.cdc.gov/nchs/nhis.htm
## Logistic regression example
## âââââââââââââââââââââââââââââââ
##   Let's predict the probability of being diagnosed with hypertension
##   based on age, sex, sleep, and bmi
str(NH11$hypev) # check stucture of hypev
levels(NH11$hypev) # check levels of hypev
# collapse all missing values to NA
NH11$hypev <- factor(NH11$hypev, levels=c("2 No", "1 Yes"))
# run our regression model
hyp.out <- glm(hypev~age_p+sex+sleep+bmi,
data=NH11, family="binomial")
coef(summary(hyp.out))
## Logistic regression coefficients
## ââââââââââââââââââââââââââââââââââââ
##   Generalized linear models use link functions, so raw coefficients are
##   difficult to interpret. For example, the age coefficient of .06 in the
##   previous model tells us that for every one unit increase in age, the
##   log odds of hypertension diagnosis increases by 0.06. Since most of us
##   are not used to thinking in log odds this is not too helpful!
##   One solution is to transform the coefficients to make them easier to
##   interpret
hyp.out.tab <- coef(summary(hyp.out))
hyp.out.tab[, "Estimate"] <- exp(coef(hyp.out))
hyp.out.tab
## Generating predicted values
## âââââââââââââââââââââââââââââââ
##   In addition to transforming the log-odds produced by `glm' to odds, we
##   can use the `predict()' function to make direct statements about the
##   predictors in our model. For example, we can ask "How much more likely
##   is a 63 year old female to have hypertension compared to a 33 year old
##   female?".
# Create a dataset with predictors set at desired levels
predDat <- with(NH11,
expand.grid(age_p = c(33, 63),
sex = "2 Female",
bmi = mean(bmi, na.rm = TRUE),
sleep = mean(sleep, na.rm = TRUE)))
# predict hypertension at those levels
cbind(predDat, predict(hyp.out, type = "response",
se.fit = TRUE, interval="confidence",
newdata = predDat))
##   This tells us that a 33 year old female has a 13% probability of
##   having been diagnosed with hypertension, while and 63 year old female
##   has a 48% probability of having been diagnosed.
## Packages for  computing and graphing predicted values
## âââââââââââââââââââââââââââââââââââââââââââââââââââââââââ
##   Instead of doing all this ourselves, we can use the effects package to
##   compute quantities of interest for us (cf. the Zelig package).
##install.packages("effects")
##library(effects)
plot(allEffects(hyp.out))
## Exercise: logistic regression
## âââââââââââââââââââââââââââââââââââ
##   Use the NH11 data set that we loaded earlier.
##   1. Use glm to conduct a logistic regression to predict ever worked
##      (everwrk) using age (age_p) and marital status (r_maritl).
##   2. Predict the probability of working for each level of marital
##      status.
##   Note that the data is not perfectly clean and ready to be modeled. You
##   will need to clean up at least some of the variables before fitting
##   the model.
summary(NH11)
View(NH11)
simple <- NH11[c("everwrk","age_p","r_maritl")]
simple
summary(NH11)
simple$everwrk <- if(simple$everwrk = "7 Refused", NA)
simple$everwrk <- if(simple$everwrk == "7 Refused", NA)
library(dplyr)
simple <- simple %>% filter(everwrk != " Refused")
summary(simple)
simple <- simple %>% filter(everwrk <> " Refused")
simple <- simple %>% filter(everwrk != " Refused")
summary(simple)
simple <- simple %>% filter(everwrk != "7 Refused")
summary(simple)
simple <- simple %>% filter(everwrk != "7 Refused" | everwrk != "9 Don't know")
summary(simple)
simple <- simple %>% filter(everwrk != "7 Refused" & everwrk != "9 Don't know")
summary(simple)
install.packages("mice")
library("mice")
set.seed(144)
imputed <- complete(mice(simple))
View(simple)
summary(simple)
simple <- NH11[c("everwrk","age_p","r_maritl")]
summary(simple)
simple[everwork == "7 Refused"] <- NA
simple$everwrk[everwork == "7 Refused"] <- NA
simple$everwrk[simple$everwork == "7 Refused"] <- NA
summary(simple)
simple$everwrk[simple$everwrk == "7 Refused"] <- NA
summary(simple)
simple$everwrk[simple$everwrk == "9 Don't know  "] <- NA
summary(simple)
simple$everwrk[simple$everwrk == "9 Don't know"] <- NA
summary(simple)
set.seed(144)
imputed <- complete(mice(simple))
summary(imputed)
NH11$everwrk<- imputed$everwrk
library(caTools)
set.seed(1000)
split <- sample.split(NH11$everwrk, SplitRatio = 0.65)
Train <- subset(NH11, split==TRUE)
Test <- subset(NH11, split==FALSE)
table(Train$everwrk)
model1 <- glm(everwrk~age_p + r_maritl, data = Train, family = "binomial")
predictTest <- predict(model1, type="response", newdata = Test)
table(Test$everwrk, predictTest>0.7)
library(ROCR)
ROCRPred <- prediction(predictTest, Test$everwrk)
simple$everwrk[simple$everwrk == "1 Yes"] <- 1
simple <- NH11[c("everwrk","age_p","r_maritl")]
simple
library(dplyr)
simple$everwrk[simple$everwrk == "7 Refused"] <- NA
simple$everwrk[simple$everwrk == "9 Don't know"] <- NA
simple$everwrk[simple$everwrk == "1 Yes"] <- 1
str(simple)
simple$everwrk[simple$everwrk == "1 Yes"] <- 1
summary(simple)
simple <- NH11[c("everwrk","age_p","r_maritl")]
simple
str(simple)
simple$everwrk[simple$everwrk == "7 Refused"] <- NA
simple$everwrk[simple$everwrk == "9 Don't know"] <- NA
summary(simple)
simple$everwrk[simple$everwrk == "7 Refused"] <- NA
simple$everwrk[simple$everwrk == "9 Don't know"] <- NA
summary(simple)
simple <- NH11[c("everwrk","age_p","r_maritl")]
simple
str(simple)
simple$everwrk[simple$everwrk == "7 Refused"] <- NA
simple$everwrk[simple$everwrk == "9 Don't know"] <- NA
summary(simple)
NH11 <- readRDS("dataSets/NatHealth2011.rds")
##   [CDC website] http://www.cdc.gov/nchs/nhis.htm
## Logistic regression example
labs <- attributes(NH11)$labels
## âââââââââââââââââââââââââââââââ
##   Let's predict the probability of being diagnosed with hypertension
##   based on age, sex, sleep, and bmi
str(NH11$hypev) # check stucture of hypev
levels(NH11$hypev) # check levels of hypev
# collapse all missing values to NA
NH11$hypev <- factor(NH11$hypev, levels=c("2 No", "1 Yes"))
# run our regression model
data=NH11, family="binomial")
hyp.out <- glm(hypev~age_p+sex+sleep+bmi,
coef(summary(hyp.out))
## Logistic regression coefficients
## ââââââââââââââââââââââââââââââââââââ
##   Generalized linear models use link functions, so raw coefficients are
##   difficult to interpret. For example, the age coefficient of .06 in the
##   previous model tells us that for every one unit increase in age, the
##   are not used to thinking in log odds this is not too helpful!
##   log odds of hypertension diagnosis increases by 0.06. Since most of us
##   One solution is to transform the coefficients to make them easier to
##   interpret
hyp.out.tab <- coef(summary(hyp.out))
hyp.out.tab[, "Estimate"] <- exp(coef(hyp.out))
hyp.out.tab
## Generating predicted values
## âââââââââââââââââââââââââââââââ
##   In addition to transforming the log-odds produced by `glm' to odds, we
##   predictors in our model. For example, we can ask "How much more likely
##   can use the `predict()' function to make direct statements about the
##   is a 63 year old female to have hypertension compared to a 33 year old
##   female?".
# Create a dataset with predictors set at desired levels
predDat <- with(NH11,
expand.grid(age_p = c(33, 63),
bmi = mean(bmi, na.rm = TRUE),
sex = "2 Female",
sleep = mean(sleep, na.rm = TRUE)))
# predict hypertension at those levels
cbind(predDat, predict(hyp.out, type = "response",
se.fit = TRUE, interval="confidence",
newdata = predDat))
##   This tells us that a 33 year old female has a 13% probability of
##   having been diagnosed with hypertension, while and 63 year old female
##   has a 48% probability of having been diagnosed.
## Packages for  computing and graphing predicted values
## âââââââââââââââââââââââââââââââââââââââââââââââââââââââââ
##   Instead of doing all this ourselves, we can use the effects package to
##   compute quantities of interest for us (cf. the Zelig package).
##install.packages("effects")
##library(effects)
plot(allEffects(hyp.out))
## Exercise: logistic regression
## âââââââââââââââââââââââââââââââââââ
##   Use the NH11 data set that we loaded earlier.
##   1. Use glm to conduct a logistic regression to predict ever worked
##      (everwrk) using age (age_p) and marital status (r_maritl).
##   2. Predict the probability of working for each level of marital
##      status.
##   Note that the data is not perfectly clean and ready to be modeled. You
##   will need to clean up at least some of the variables before fitting
##   the model.
summary(NH11)
simple <- NH11[c("everwrk","age_p","r_maritl")]
simple
str(simple)
simple$everwrk[simple$everwrk == "7 Refused"] <- NA
simple$everwrk[simple$everwrk == "9 Don't know"] <- NA
summary(simple)
simple$everwrk <- factor(simple$everwrk)
summary(simple)
library("mice")
set.seed(144)
imputed <- complete(mice(simple))
summary(imputed)
NH11$everwrk<- imputed$everwrk
NH11$everwrk<- imputed$everwrk
library(caTools)
set.seed(1000)
split <- sample.split(NH11$everwrk, SplitRatio = 0.65)
Train <- subset(NH11, split==TRUE)
Test <- subset(NH11, split==FALSE)
table(Train$everwrk)
model1 <- glm(everwrk~age_p + r_maritl, data = Train, family = "binomial")
predictTest <- predict(model1, type="response", newdata = Test)
table(Test$everwrk, predictTest>0.7)
install.packages("ROCR")
install.packages("ROCR")
library(ROCR)
ROCRPred <- prediction(predictTest, Test$everwrk)
ROCRPred <- prediction(predictTest, Test$everwrk)
perf <- performance(ROCRPred, measure = "tpr", x.measure = "fpr")
plot(perf, col = rainbow(10))
auc <- performance(ROCRPred, measure = "auc")
auc <- auc@y.values[[1]]
auc
## Regression with binary outcomes
## âââââââââââââââââââââââââââââââââ
## Logistic regression
## âââââââââââââââââââââââ
##   This far we have used the `lm' function to fit our regression models.
##   `lm' is great, but limitedâin particular it only fits models for
##   continuous dependent variables. For categorical dependent variables we
##   can use the `glm()' function.
##   For these models we will use a different dataset, drawn from the
##   National Health Interview Survey. From the [CDC website]:
##         The National Health Interview Survey (NHIS) has monitored
##         the health of the nation since 1957. NHIS data on a broad
##         range of health topics are collected through personal
##         household interviews. For over 50 years, the U.S. Census
##         Bureau has been the data collection agent for the National
##         Health Interview Survey. Survey results have been
##         instrumental in providing data to track health status,
##         health care access, and progress toward achieving national
##         health objectives.
##   Load the National Health Interview Survey data:
setwd("C:/Users/Noura/Documents/R/Data Science Foundation/logistic_regression/logistic_regression")
NH11 <- readRDS("dataSets/NatHealth2011.rds")
labs <- attributes(NH11)$labels
##   [CDC website] http://www.cdc.gov/nchs/nhis.htm
## Logistic regression example
## âââââââââââââââââââââââââââââââ
##   Let's predict the probability of being diagnosed with hypertension
##   based on age, sex, sleep, and bmi
str(NH11$hypev) # check stucture of hypev
levels(NH11$hypev) # check levels of hypev
# collapse all missing values to NA
NH11$hypev <- factor(NH11$hypev, levels=c("2 No", "1 Yes"))
# run our regression model
hyp.out <- glm(hypev~age_p+sex+sleep+bmi,
data=NH11, family="binomial")
coef(summary(hyp.out))
## Logistic regression coefficients
## ââââââââââââââââââââââââââââââââââââ
##   Generalized linear models use link functions, so raw coefficients are
##   difficult to interpret. For example, the age coefficient of .06 in the
##   previous model tells us that for every one unit increase in age, the
##   log odds of hypertension diagnosis increases by 0.06. Since most of us
##   are not used to thinking in log odds this is not too helpful!
##   One solution is to transform the coefficients to make them easier to
##   interpret
hyp.out.tab <- coef(summary(hyp.out))
hyp.out.tab[, "Estimate"] <- exp(coef(hyp.out))
hyp.out.tab
## Generating predicted values
## âââââââââââââââââââââââââââââââ
##   In addition to transforming the log-odds produced by `glm' to odds, we
##   can use the `predict()' function to make direct statements about the
##   predictors in our model. For example, we can ask "How much more likely
##   is a 63 year old female to have hypertension compared to a 33 year old
##   female?".
# Create a dataset with predictors set at desired levels
predDat <- with(NH11,
expand.grid(age_p = c(33, 63),
sex = "2 Female",
bmi = mean(bmi, na.rm = TRUE),
sleep = mean(sleep, na.rm = TRUE)))
# predict hypertension at those levels
cbind(predDat, predict(hyp.out, type = "response",
se.fit = TRUE, interval="confidence",
newdata = predDat))
##   This tells us that a 33 year old female has a 13% probability of
##   having been diagnosed with hypertension, while and 63 year old female
##   has a 48% probability of having been diagnosed.
## Packages for  computing and graphing predicted values
## âââââââââââââââââââââââââââââââââââââââââââââââââââââââââ
##   Instead of doing all this ourselves, we can use the effects package to
##   compute quantities of interest for us (cf. the Zelig package).
##install.packages("effects")
##library(effects)
plot(allEffects(hyp.out))
## Exercise: logistic regression
## âââââââââââââââââââââââââââââââââââ
##   Use the NH11 data set that we loaded earlier.
##   1. Use glm to conduct a logistic regression to predict ever worked
##      (everwrk) using age (age_p) and marital status (r_maritl).
##   2. Predict the probability of working for each level of marital
##      status.
##   Note that the data is not perfectly clean and ready to be modeled. You
##   will need to clean up at least some of the variables before fitting
##   the model.
summary(NH11)
simple <- NH11[c("everwrk","age_p","r_maritl")]
simple
str(simple)
simple$everwrk[simple$everwrk == "7 Refused"] <- NA
simple$everwrk[simple$everwrk == "9 Don't know"] <- NA
simple$everwrk <- factor(simple$everwrk)
summary(simple)
install.packages("mice")
library("mice")
set.seed(144)
imputed <- complete(mice(simple))
summary(imputed)
NH11$everwrk<- imputed$everwrk
library(caTools)
set.seed(1000)
split <- sample.split(NH11$everwrk, SplitRatio = 0.65)
Train <- subset(NH11, split==TRUE)
Test <- subset(NH11, split==FALSE)
table(Train$everwrk)
model1 <- glm(everwrk~age_p + r_maritl, data = Train, family = "binomial")
predictTest <- predict(model1, type="response", newdata = Test)
table(Test$everwrk, predictTest>0.7)
install.packages("ROCR")
library(ROCR)
ROCRPred <- prediction(predictTest, Test$everwrk)
perf <- performance(ROCRPred, measure = "tpr", x.measure = "fpr")
plot(perf, col = rainbow(10))
auc <- performance(ROCRPred, measure = "auc")
auc <- auc@y.values[[1]]
auc
install.packages("mice")
install.packages("mice")
summary(NH11)
simple <- NH11[c("everwrk","age_p","r_maritl")]
simple
str(simple)
simple$everwrk[simple$everwrk == "7 Refused"] <- NA
simple$everwrk[simple$everwrk == "9 Don't know"] <- NA
simple$everwrk <- factor(simple$everwrk)
summary(simple)
install.packages("mice")
library("mice")
set.seed(144)
imputed <- complete(mice(simple))
summary(imputed)
NH11$everwrk<- imputed$everwrk
library(caTools)
set.seed(1000)
split <- sample.split(NH11$everwrk, SplitRatio = 0.65)
Train <- subset(NH11, split==TRUE)
Test <- subset(NH11, split==FALSE)
table(Train$everwrk)
model1 <- glm(everwrk~age_p + r_maritl, data = Train, family = "binomial")
predictTest <- predict(model1, type="response", newdata = Test)
table(Test$everwrk, predictTest>0.7)
install.packages("ROCR")
library(ROCR)
ROCRPred <- prediction(predictTest, Test$everwrk)
perf <- performance(ROCRPred, measure = "tpr", x.measure = "fpr")
plot(perf, col = rainbow(10))
auc <- performance(ROCRPred, measure = "auc")
auc <- auc@y.values[[1]]
auc
install.packages("mice")
install.packages("ROCR")
